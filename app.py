import streamlit as st
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from io import StringIO
import re
from urllib.parse import urlparse

# ---------------- Page Setup ----------------
st.set_page_config(page_title="App-ads.txt Health Checker", layout="wide")
st.title("üõ°Ô∏è App-ads.txt Health & Line Counter")
st.markdown("""
–≠—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ **app-ads.txt**, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã, 
–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—à–∏–±–æ–∫ (Soft 404) –∏ —Å—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫.
""")

# ---------------- Input Tabs ----------------
tab1, tab2 = st.tabs(["üìã –í—Å—Ç–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫", "üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"])

domains = []
with tab1:
    st.header("–°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤")
    domain_input = st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ –¥–æ–º–µ–Ω—ã (–æ–¥–∏–Ω –Ω–∞ —Å—Ç—Ä–æ–∫—É, –Ω–∞–ø—Ä–∏–º–µ—Ä: site.com)", height=200)
    if domain_input:
        domains = [d.strip() for d in domain_input.splitlines() if d.strip()]

with tab2:
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞")
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ TXT —Ñ–∞–π–ª", type=["csv", "txt"])
    if uploaded_file:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        uploaded_domains = [line.strip() for line in stringio.readlines() if line.strip()]
        domains.extend(uploaded_domains)

# Deduplicate
domains = list(dict.fromkeys(domains))
if domains:
    st.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤: {len(domains)}")

# ---------------- Settings Sidebar ----------------
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

timeout_sec = st.sidebar.slider("–¢–∞–π–º-–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (—Å–µ–∫)", 3, 20, 8)
max_threads = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤", 5, 50, 20)
ua_mode = st.sidebar.radio("–†–µ–∂–∏–º User-Agent", ["Chrome (Windows)", "Google Bot"])

if ua_mode == "Chrome (Windows)":
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
else:
    USER_AGENT = "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"

# ---------------- Logic Functions ----------------

def clean_domain(raw_url):
    """–û—á–∏—â–∞–µ—Ç –≤–≤–æ–¥, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –¥–æ–º–µ–Ω –∏–ª–∏ –±–∞–∑–æ–≤—ã–π URL –±–µ–∑ —Ö–≤–æ—Å—Ç–æ–≤."""
    raw_url = raw_url.strip()
    if not raw_url.startswith("http"):
        raw_url = "http://" + raw_url
    try:
        parsed = urlparse(raw_url)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º netloc (–¥–æ–º–µ–Ω) + path (–µ—Å–ª–∏ —Å–∞–π—Ç –≤ –ø–∞–ø–∫–µ), –Ω–æ –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–µ—à–µ–π
        return parsed.netloc + parsed.path.rstrip('/')
    except:
        return raw_url

def analyze_content(text):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Ñ–∞–π–ª–∞: —Å—á–∏—Ç–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏, –∏—â–µ—Ç HTML –º—É—Å–æ—Ä."""
    if not text:
        return 0, "Empty File"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Soft 404 (HTML –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–∞)
    text_lower = text.lower()[:500] # –°–º–æ—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    if "<!doctype html" in text_lower or "<html" in text_lower or "<body" in text_lower or "<div" in text_lower:
        return 0, "Soft 404 (HTML)"

    # –°—á–∏—Ç–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (–Ω–µ –ø—É—Å—Ç—ã–µ, –Ω–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏)
    lines = text.splitlines()
    valid_count = 0
    for line in lines:
        line = line.strip()
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if line and not line.startswith('#'):
            valid_count += 1
    
    if valid_count == 0:
        return 0, "Empty File"
    
    return valid_count, "Valid"

def check_domain(domain):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    clean_d = clean_domain(domain)
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π URL
    target_url = f"https://{clean_d}/app-ads.txt"
    # –ï—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç HTTPS, –ø–æ–ø—Ä–æ–±—É–µ–º HTTP –≤–Ω—É—Ç—Ä–∏ —Å–µ—Å—Å–∏–∏, –Ω–æ –Ω–∞—á–Ω–µ–º —Å HTTPS
    
    session = requests.Session()
    session.headers.update({'User-Agent': USER_AGENT})
    
    result = {
        "Input Domain": domain,
        "Final URL": target_url,
        "Status": "Unknown",
        "Code": 0,
        "Lines": 0
    }

    try:
        # –ü–æ–ø—ã—Ç–∫–∞ 1: HTTPS
        response = session.get(target_url, timeout=timeout_sec, allow_redirects=True)
    except requests.exceptions.SSLError:
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –ï—Å–ª–∏ SSL –æ—à–∏–±–∫–∞, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
        try:
            response = session.get(target_url, timeout=timeout_sec, allow_redirects=True, verify=False)
        except Exception as e:
             # –ü–æ–ø—ã—Ç–∫–∞ 3: –ü—Ä–æ–±—É–µ–º HTTP –µ—Å–ª–∏ —Å–æ–≤—Å–µ–º –ø–ª–æ—Ö–æ
            try:
                target_url_http = f"http://{clean_d}/app-ads.txt"
                response = session.get(target_url_http, timeout=timeout_sec, allow_redirects=True)
                result["Final URL"] = response.url
            except Exception as e_final:
                result["Status"] = "Connection Error"
                result["Code"] = "ERR"
                return result
    except Exception as e:
        result["Status"] = "Connection Error"
        result["Code"] = "ERR"
        return result

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
    result["Code"] = response.status_code
    result["Final URL"] = response.url

    if response.status_code == 200:
        count, status_msg = analyze_content(response.text)
        result["Lines"] = count
        result["Status"] = status_msg
    elif response.status_code == 403:
        result["Status"] = "Forbidden"
    elif response.status_code == 404:
        result["Status"] = "Not Found"
    else:
        result["Status"] = f"HTTP {response.status_code}"

    return result

# ---------------- Main Execution ----------------

if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É", disabled=not domains):
    start_time = time.time()
    results_data = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        future_to_domain = {executor.submit(check_domain, d): d for d in domains}
        
        for i, future in enumerate(as_completed(future_to_domain)):
            data = future.result()
            results_data.append(data)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            percent = (i + 1) / len(domains)
            progress_bar.progress(percent)
            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –∏–∑ {len(domains)}...")

    end_time = time.time()
    st.success(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ó–∞—Ç—Ä–∞—á–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–∏: {end_time - start_time:.2f} —Å–µ–∫.")

    # ---------------- Display Results ----------------
    df = pd.DataFrame(results_data)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –°–Ω–∞—á–∞–ª–∞ Valid, –ø–æ—Ç–æ–º –æ—à–∏–±–∫–∏
    df = df.sort_values(by=["Lines"], ascending=False)
    
    # –í–∏–∑—É–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    def highlight_status(val):
        color = 'black'
        if val == 'Valid':
            color = 'green'
        elif val == 'Not Found' or val == 'Empty File':
            color = 'red'
        elif 'Soft 404' in val:
            color = 'orange'
        return f'color: {color}; font-weight: bold'

    st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏")
    st.dataframe(
        df.style.map(highlight_status, subset=['Status']),
        use_container_width=True,
        column_config={
            "Final URL": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª"),
            "Lines": st.column_config.NumberColumn("–ö–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫", format="%d")
        }
    )

    # ---------------- Export ----------------
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç (CSV)",
        data=csv,
        file_name='app_ads_report.csv',
        mime='text/csv',
    )
